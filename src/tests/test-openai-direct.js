/**
 * üß™ TESTE OPENAI DIRETO - FASE 3
 * 
 * Testa apenas as funcionalidades b√°sicas da OpenAI
 * sem depend√™ncias de Firestore complexas
 */

require('dotenv').config();
const OpenAI = require('openai');

async function testOpenAIDirect() {
    console.log('üöÄ TESTANDO OPENAI DIRETAMENTE - FASE 3\n');

    try {
        // 1. Testar conex√£o b√°sica
        console.log('üîå 1. Testando conex√£o OpenAI...');
        
        const openai = new OpenAI({
            apiKey: process.env.OPENAI_API_KEY,
        });

        const testResponse = await openai.chat.completions.create({
            model: 'gpt-4o-mini',
            messages: [
                {
                    role: 'user',
                    content: 'Responda apenas "OK" se voc√™ est√° funcionando.'
                }
            ],
            max_tokens: 10,
            temperature: 0
        });

        const response = testResponse.choices[0].message.content;
        console.log(`‚úÖ Resposta OpenAI: "${response}"`);

        // 2. Testar categoriza√ß√£o de mensagem
        console.log('\nüè∑Ô∏è 2. Testando categoriza√ß√£o de mensagem...');
        
        const messageToAnalyze = 'Ol√°, gostaria de fazer um pedido de pizza, mas estou com problema no pagamento';
        
        const categorizationResponse = await openai.chat.completions.create({
            model: 'gpt-4o-mini',
            messages: [
                {
                    role: 'system',
                    content: 'Voc√™ √© um especialista em categoriza√ß√£o de mensagens. Sempre responda com JSON v√°lido.'
                },
                {
                    role: 'user',
                    content: `
MENSAGEM PARA CATEGORIZAR: "${messageToAnalyze}"

RETORNE UM JSON com:
{
  "category": "suporte|vendas|reclamacao|duvida|elogio|outros",
  "urgency": "low|normal|high|urgent", 
  "sentiment": "positive|neutral|negative",
  "topics": ["topico1", "topico2"],
  "confidence": 0.95
}

Apenas o JSON:`
                }
            ],
            max_tokens: 200,
            temperature: 0.1
        });

        const categorizationText = categorizationResponse.choices[0].message.content;
        console.log('üìä Categoriza√ß√£o:', categorizationText);

        try {
            const categorization = JSON.parse(categorizationText);
            console.log('‚úÖ Categoriza√ß√£o JSON v√°lida!');
            console.log(`   Categoria: ${categorization.category}`);
            console.log(`   Urg√™ncia: ${categorization.urgency}`);
            console.log(`   Sentimento: ${categorization.sentiment}`);
        } catch (parseError) {
            console.log('‚ö†Ô∏è JSON inv√°lido, mas resposta recebida');
        }

        // 3. Testar gera√ß√£o de resumo
        console.log('\nüìù 3. Testando gera√ß√£o de resumo...');
        
        const conversationsToSummarize = [
            'Cliente: Ol√°, gostaria de fazer um pedido',
            'Atendente: Claro! O que voc√™ gostaria?',
            'Cliente: Uma pizza margherita grande',
            'Atendente: Perfeito! Qual o endere√ßo para entrega?',
            'Cliente: Rua das Flores, 123',
            'Atendente: Ser√° 45 minutos. Total: R$ 35,00'
        ];

        const summaryResponse = await openai.chat.completions.create({
            model: 'gpt-4o-mini',
            messages: [
                {
                    role: 'system',
                    content: 'Voc√™ √© especialista em resumir conversas de atendimento ao cliente.'
                },
                {
                    role: 'user',
                    content: `
CONVERSAS PARA RESUMIR:
${conversationsToSummarize.join('\n')}

Crie um resumo conciso de at√© 100 palavras destacando:
- O que o cliente pediu
- Informa√ß√µes importantes
- Resultado da conversa

RESUMO:`
                }
            ],
            max_tokens: 300,
            temperature: 0.3
        });

        const summary = summaryResponse.choices[0].message.content;
        console.log('üìã Resumo gerado:', summary);

        // 4. Testar resposta contextual
        console.log('\nüí¨ 4. Testando resposta contextual...');
        
        const contextualResponse = await openai.chat.completions.create({
            model: 'gpt-4o-mini',
            messages: [
                {
                    role: 'system',
                    content: 'Voc√™ √© um atendente da Pizzaria do Jo√£o. Seja amig√°vel e prestativo.'
                },
                {
                    role: 'user',
                    content: `
CONTEXTO DO CLIENTE:
Nome: Maria Silva
Hist√≥rico: Cliente frequente, prefere pizza margherita, sempre pede aos s√°bados

MENSAGEM ATUAL DO CLIENTE:
"Oi! Estou pensando em fazer um pedido para hoje"

Responda como atendente da pizzaria, usando o contexto:`
                }
            ],
            max_tokens: 200,
            temperature: 0.7
        });

        const contextualAnswer = contextualResponse.choices[0].message.content;
        console.log('üéØ Resposta contextual:', contextualAnswer);

        // 5. Resultados finais
        console.log('\n' + '='.repeat(50));
        console.log('üéâ TESTE OPENAI DIRETO - CONCLU√çDO!');
        console.log('='.repeat(50));
        console.log('‚úÖ Conex√£o OpenAI: OK');
        console.log('‚úÖ Categoriza√ß√£o: OK');
        console.log('‚úÖ Resumos: OK');
        console.log('‚úÖ Respostas contextuais: OK');
        console.log('\nüöÄ FASE 3 - INTEGRA√á√ÉO OPENAI FUNCIONAL!');
        console.log('‚ú® Pronta para implementar FASE 4: Agente IA Inteligente');

    } catch (error) {
        console.error('\n‚ùå ERRO NO TESTE OPENAI:', error);
        
        if (error.message.includes('API key')) {
            console.log('\nüí° DICA: Verifique se a OPENAI_API_KEY est√° configurada no .env');
        }
        
        throw error;
    }
}

// Executar teste
if (require.main === module) {
    testOpenAIDirect().catch(console.error);
}

module.exports = testOpenAIDirect;
