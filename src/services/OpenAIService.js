const OpenAI = require('openai');
require('dotenv').config();

/**
 * ü§ñ OPENAI SERVICE
 * Servi√ßo completo para integra√ß√£o com OpenAI GPT
 * Focado em resumos autom√°ticos e an√°lise de conversas
 */

class OpenAIService {
  constructor() {
    this.openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY
    });
    
    // Configura√ß√µes padr√£o
    this.defaultModel = 'gpt-4o-mini'; // Modelo mais econ√¥mico e eficiente
    this.maxTokens = 2000;
    this.temperature = 0.7;
    
    console.log('ü§ñ OpenAI Service initialized');
  }

  /**
   * Gerar resumo de conversas antigas
   * @param {Array} conversations - Array de conversas para resumir
   * @param {object} clientInfo - Informa√ß√µes do cliente
   * @param {object} companyInfo - Informa√ß√µes da empresa
   * @returns {Promise<string>} - Resumo gerado
   */
  async generateConversationSummary(conversations, clientInfo, companyInfo) {
    try {
      if (!conversations || conversations.length === 0) {
        return 'Nenhuma conversa para resumir.';
      }

      // Preparar contexto das conversas
      const conversationText = this.formatConversationsForSummary(conversations);
      
      const prompt = `
CONTEXTO DA EMPRESA:
Nome: ${companyInfo.name}
Tipo: ${companyInfo.domain}

CLIENTE:
Nome: ${clientInfo.name}
Contato: ${clientInfo.contact?.whatsapp || clientInfo.contact?.email || 'N/A'}

INSTRU√á√ïES:
Voc√™ √© um assistente especializado em resumir conversas de atendimento ao cliente.
Analise as conversas abaixo e crie um resumo estruturado e √∫til.

CONVERSAS PARA RESUMIR:
${conversationText}

FORMATO DO RESUMO:
Crie um resumo seguindo esta estrutura:

**RESUMO DO CLIENTE:**
- Nome: [nome do cliente]
- Per√≠odo: [per√≠odo das conversas]
- Total de intera√ß√µes: [n√∫mero]

**PRINCIPAIS T√ìPICOS:**
- [t√≥pico 1]: [breve descri√ß√£o]
- [t√≥pico 2]: [breve descri√ß√£o]
- [t√≥pico 3]: [breve descri√ß√£o]

**PREFER√äNCIAS IDENTIFICADAS:**
- [prefer√™ncia 1]
- [prefer√™ncia 2]
- [prefer√™ncia 3]

**PADR√ÉO DE COMPORTAMENTO:**
[descri√ß√£o do comportamento do cliente baseado nas conversas]

**HIST√ìRICO DE QUEST√ïES:**
- [quest√£o resolvida 1]
- [quest√£o resolvida 2]
- [quest√£o pendente se houver]

**RECOMENDA√á√ïES PARA PR√ìXIMAS INTERA√á√ïES:**
- [recomenda√ß√£o 1]
- [recomenda√ß√£o 2]

Seja conciso mas informativo. Foque em informa√ß√µes √∫teis para futuras intera√ß√µes.
`;

      const response = await this.openai.chat.completions.create({
        model: this.defaultModel,
        messages: [
          {
            role: 'system',
            content: 'Voc√™ √© um especialista em resumir conversas de atendimento ao cliente, extraindo insights valiosos para melhorar o relacionamento.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        max_tokens: this.maxTokens,
        temperature: this.temperature
      });

      const summary = response.choices[0].message.content;
      
      console.log(`‚úÖ Summary generated for client ${clientInfo.name}: ${summary.length} characters`);
      return summary;

    } catch (error) {
      console.error('‚ùå Error generating conversation summary:', error);
      throw error;
    }
  }

  /**
   * Gerar resumo de perfil do cliente baseado em hist√≥rico
   * @param {object} clientData - Dados completos do cliente
   * @param {Array} recentSummaries - Resumos recentes
   * @param {object} companyInfo - Informa√ß√µes da empresa
   * @returns {Promise<object>} - Perfil resumido
   */
  async generateClientProfile(clientData, recentSummaries = [], companyInfo) {
    try {
      const summariesText = recentSummaries.join('\n\n');
      
      const prompt = `
CONTEXTO DA EMPRESA:
Nome: ${companyInfo.name}
Setor: ${companyInfo.domain}

DADOS DO CLIENTE:
Nome: ${clientData.name}
Contato: ${JSON.stringify(clientData.contact, null, 2)}
Tags atuais: ${clientData.tags?.join(', ') || 'Nenhuma'}
Resumo atual: ${clientData.summary || 'N√£o dispon√≠vel'}

HIST√ìRICO DE RESUMOS:
${summariesText || 'Nenhum resumo dispon√≠vel'}

INSTRU√á√ïES:
Analise todos os dados dispon√≠veis e crie um perfil atualizado do cliente.
Identifique padr√µes, prefer√™ncias e caracter√≠sticas importantes.

FORMATO DA RESPOSTA (JSON):
{
  "profile": {
    "personalityType": "string (amig√°vel/formal/direto/etc)",
    "preferences": ["pref1", "pref2", "pref3"],
    "behaviorPattern": "string (descri√ß√£o do comportamento)",
    "communicationStyle": "string (como prefere se comunicar)",
    "frequency": "string (frequ√™ncia de contato)",
    "topics": ["t√≥pico1", "t√≥pico2", "t√≥pico3"]
  },
  "summary": "string (resumo atualizado em portugu√™s)",
  "tags": ["tag1", "tag2", "tag3"],
  "insights": [
    "insight √∫til 1",
    "insight √∫til 2"
  ],
  "recommendations": [
    "recomenda√ß√£o para pr√≥ximas intera√ß√µes"
  ]
}

Responda APENAS com o JSON v√°lido, sem texto adicional.
`;

      const response = await this.openai.chat.completions.create({
        model: this.defaultModel,
        messages: [
          {
            role: 'system',
            content: 'Voc√™ √© um especialista em an√°lise de perfil de clientes. Sempre responda com JSON v√°lido.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        max_tokens: 1500,
        temperature: 0.3 // Menos criativo para estrutura consistente
      });

      const profileText = response.choices[0].message.content;
      const profile = JSON.parse(profileText);
      
      console.log(`‚úÖ Client profile generated for ${clientData.name}`);
      return profile;

    } catch (error) {
      console.error('‚ùå Error generating client profile:', error);
      
      // Fallback em caso de erro
      return {
        profile: {
          personalityType: 'an√°lise_pendente',
          preferences: [],
          behaviorPattern: 'Dados insuficientes para an√°lise',
          communicationStyle: 'A definir',
          frequency: 'Irregular',
          topics: []
        },
        summary: clientData.summary || 'Cliente sem hist√≥rico suficiente para an√°lise detalhada.',
        tags: clientData.tags || [],
        insights: ['Necess√°rio mais intera√ß√µes para an√°lise precisa'],
        recommendations: ['Coletar mais informa√ß√µes nas pr√≥ximas conversas']
      };
    }
  }

  /**
   * Categorizar mensagem automaticamente
   * @param {string} messageContent - Conte√∫do da mensagem
   * @param {object} context - Contexto adicional
   * @returns {Promise<object>} - Categoriza√ß√£o
   */
  async categorizeMessage(messageContent, context = {}) {
    try {
      const prompt = `
EMPRESA: ${context.companyName || 'N/A'}
CLIENTE: ${context.clientName || 'N/A'}

MENSAGEM PARA CATEGORIZAR:
"${messageContent}"

INSTRU√á√ïES:
Analise a mensagem e categorize conforme as op√ß√µes abaixo.

CATEGORIAS DISPON√çVEIS:
- vendas: interesse em produtos/servi√ßos
- suporte: problemas t√©cnicos ou d√∫vidas
- entrega: quest√µes de envio/recebimento
- pagamento: quest√µes financeiras/cobran√ßa
- feedback: elogios/reclama√ß√µes/sugest√µes
- informacoes: pedidos de informa√ß√£o geral
- agendamento: marca√ß√£o de hor√°rios/consultas
- cancelamento: cancelamentos de produtos/servi√ßos
- geral: n√£o se encaixa nas outras categorias

PRIORIDADES:
- low: informa√ß√µes gerais, elogios
- normal: vendas, agendamentos, d√∫vidas simples
- high: problemas, reclama√ß√µes, cancelamentos
- urgent: emerg√™ncias, problemas cr√≠ticos

FORMATO DA RESPOSTA (JSON):
{
  "category": "categoria_escolhida",
  "priority": "prioridade_escolhida", 
  "confidence": 0.95,
  "tags": ["tag1", "tag2"],
  "sentiment": "positive|negative|neutral",
  "summary": "breve resumo da mensagem"
}

Responda APENAS com JSON v√°lido.
`;

      const response = await this.openai.chat.completions.create({
        model: this.defaultModel,
        messages: [
          {
            role: 'system',
            content: 'Voc√™ √© um especialista em categoriza√ß√£o de mensagens de atendimento. Sempre responda com JSON v√°lido.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        max_tokens: 500,
        temperature: 0.1 // Muito baixo para consist√™ncia
      });

      const categorizationText = response.choices[0].message.content;
      const categorization = JSON.parse(categorizationText);
      
      console.log(`‚úÖ Message categorized: ${categorization.category} (${categorization.priority})`);
      return categorization;

    } catch (error) {
      console.error('‚ùå Error categorizing message:', error);
      
      // Fallback simples
      return {
        category: 'geral',
        priority: 'normal',
        confidence: 0.1,
        tags: ['automatico'],
        sentiment: 'neutral',
        summary: 'Categoriza√ß√£o autom√°tica falhou'
      };
    }
  }

  /**
   * Gerar resposta contextual para o cliente
   * @param {string} message - Mensagem do cliente
   * @param {object} context - Contexto completo
   * @returns {Promise<string>} - Resposta sugerida
   */
  async generateContextualResponse(message, context) {
    try {
      const {
        company,
        client,
        recentConversations = [],
        clientSummary = ''
      } = context;

      // Formatar conversas recentes
      const recentHistory = recentConversations.slice(0, 10).map(conv => 
        `[${conv.role}]: ${conv.content}`
      ).join('\n');

      const prompt = `
CONTEXTO DA EMPRESA:
Nome: ${company.name}
Prompt customizado: ${company.customPrompt || 'Seja sempre educado e prestativo'}

INFORMA√á√ïES DO CLIENTE:
Nome: ${client.name}
Resumo do perfil: ${clientSummary}
Tags: ${client.tags?.join(', ') || 'Nenhuma'}

HIST√ìRICO RECENTE (√∫ltimas mensagens):
${recentHistory}

NOVA MENSAGEM DO CLIENTE:
"${message}"

INSTRU√á√ïES:
1. Responda como se fosse um atendente da ${company.name}
2. Use as informa√ß√µes do cliente para personalizar a resposta
3. Mantenha consist√™ncia com o hist√≥rico recente
4. Seja natural, prestativo e profissional
5. Se necess√°rio, fa√ßa perguntas de esclarecimento
6. N√ÉO mencione que voc√™ √© uma IA

RESPOSTA:
`;

      const response = await this.openai.chat.completions.create({
        model: this.defaultModel,
        messages: [
          {
            role: 'system',
            content: `Voc√™ √© um atendente experiente da empresa ${company.name}. Sempre mantenha o tom profissional mas amig√°vel, e use o contexto do cliente para personalizar suas respostas.`
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        max_tokens: 800,
        temperature: 0.7
      });

      const suggestedResponse = response.choices[0].message.content;
      
      console.log(`‚úÖ Contextual response generated for ${client.name}`);
      return suggestedResponse.trim();

    } catch (error) {
      console.error('‚ùå Error generating contextual response:', error);
      throw error;
    }
  }

  /**
   * Busca sem√¢ntica simples em resumos
   * @param {string} query - Consulta de busca
   * @param {Array} summaries - Array de resumos para buscar
   * @returns {Promise<Array>} - Resumos relevantes ordenados por relev√¢ncia
   */
  async semanticSearch(query, summaries) {
    try {
      if (!summaries || summaries.length === 0) {
        return [];
      }

      const prompt = `
CONSULTA DE BUSCA: "${query}"

RESUMOS DISPON√çVEIS:
${summaries.map((summary, index) => `${index}: ${summary.text || summary}`).join('\n\n')}

INSTRU√á√ïES:
Analise quais resumos s√£o mais relevantes para a consulta.
Retorne os IDs dos resumos ordenados por relev√¢ncia (mais relevante primeiro).

FORMATO DA RESPOSTA (JSON):
{
  "relevant_ids": [2, 5, 1],
  "explanation": "breve explica√ß√£o do crit√©rio usado"
}

Se nenhum resumo for relevante, retorne array vazio.
Responda APENAS com JSON v√°lido.
`;

      const response = await this.openai.chat.completions.create({
        model: this.defaultModel,
        messages: [
          {
            role: 'system',
            content: 'Voc√™ √© um especialista em busca sem√¢ntica. Sempre responda com JSON v√°lido.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        max_tokens: 300,
        temperature: 0.1
      });

      const searchResult = JSON.parse(response.choices[0].message.content);
      const relevantSummaries = searchResult.relevant_ids.map(id => summaries[id]).filter(Boolean);
      
      console.log(`‚úÖ Semantic search completed: ${relevantSummaries.length} relevant results`);
      return relevantSummaries;

    } catch (error) {
      console.error('‚ùå Error in semantic search:', error);
      return [];
    }
  }

  /**
   * Helper: Formatar conversas para resumo
   * @private
   */
  formatConversationsForSummary(conversations) {
    return conversations.map((conv, index) => {
      const date = conv.timestamp ? new Date(conv.timestamp.seconds * 1000).toLocaleDateString('pt-BR') : 'Data n√£o dispon√≠vel';
      const content = conv.content || conv.text || 'Conte√∫do n√£o dispon√≠vel';
      const role = conv.role || 'user';
      
      return `${index + 1}. [${date}] [${role.toUpperCase()}]: ${content}`;
    }).join('\n');
  }

  /**
   * Validar configura√ß√£o do servi√ßo
   * @returns {Promise<boolean>} - Status da valida√ß√£o
   */
  async validateService() {
    try {
      const response = await this.openai.chat.completions.create({
        model: this.defaultModel,
        messages: [
          {
            role: 'user',
            content: 'Responda apenas "OK" se voc√™ est√° funcionando corretamente.'
          }
        ],
        max_tokens: 10,
        temperature: 0
      });

      const isValid = response.choices[0].message.content.trim().toLowerCase().includes('ok');
      
      if (isValid) {
        console.log('‚úÖ OpenAI Service validation successful');
      } else {
        console.log('‚ö†Ô∏è OpenAI Service validation failed');
      }
      
      return isValid;

    } catch (error) {
      console.error('‚ùå OpenAI Service validation error:', error);
      return false;
    }
  }
}

module.exports = new OpenAIService();
